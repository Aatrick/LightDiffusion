{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About LightDiffusion","text":"<p>Welcome to LightDiffusion, a cutting-edge tool designed to generate high-quality images from text prompts. This project aims to simplify and enhance the process of creating stunning visuals using advanced machine learning techniques.</p>"},{"location":"#project-overview","title":"Project Overview","text":"<p>LightDiffusion is built on the principles of stability, efficiency, and user-friendliness. By leveraging state-of-the-art models and algorithms, it provides users with the ability to generate detailed and high-resolution images with minimal effort.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Txt2Img and Img2Img: Generate images from text prompts or transform existing images.</li> <li>Attention Syntax: Fine-tune the focus of your prompts for more precise results.</li> <li>Hires-Fix: Enhance the resolution of generated images while preserving details.</li> <li>GPU Optimization: Utilize GPU acceleration for faster image generation.</li> <li>Xformers and Pytorch Optimization: Improved performance and efficiency.</li> <li>Stable-Fast Implementation: Achieve up to 70% speedup with optimized model inference.</li> <li>FP16 and FP32 Precision Support: Choose between different precision levels for your models.</li> <li>GUI: User-friendly graphical interface for easy interaction.</li> <li>Automatic Prompt-Enhancing: Enhance your prompts automatically with advanced algorithms.</li> <li>Discord Bot Integration: Use LightDiffusion directly within Discord.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>To get started with LightDiffusion, follow the installation guide which provides detailed steps for setting up the project on your system.</p>"},{"location":"#usage","title":"Usage","text":"<p>LightDiffusion offers a variety of features and customization options. Refer to the Prompting Guide and HiresFix &amp; Adetailer Guide for detailed instructions on how to use these features effectively.</p>"},{"location":"#community-and-support","title":"Community and Support","text":"<p>Join our community on GitHub to share your creations, report issues, and contribute to the project. Check out the FAQ for common questions and troubleshooting tips.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions from the community. If you\u2019re interested in contributing, please read our contributing guidelines to get started.</p>"},{"location":"#license","title":"License","text":"<p>LightDiffusion is released under the MIT License. Feel free to use, modify, and distribute this software in accordance with the license terms.</p> <p>Thank you for using LightDiffusion! We hope you enjoy creating amazing visuals with our tool.</p>"},{"location":"CODE_OF_CONDUCT/","title":"Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#introduction","title":"Introduction","text":"<p>Welcome to the LightDiffusion community! We are committed to providing a friendly, safe, and welcoming environment for all, regardless of gender, sexual orientation, disability, ethnicity, religion, or age. This Code of Conduct outlines our expectations for participants within the community, as well as the steps to reporting unacceptable behavior.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":""},{"location":"CODE_OF_CONDUCT/#be-respectful","title":"Be Respectful","text":"<ul> <li>Treat everyone with respect. </li> <li>Be considerate of differing viewpoints and experiences.</li> <li>Gracefully accept constructive criticism.</li> </ul>"},{"location":"CODE_OF_CONDUCT/#be-inclusive","title":"Be Inclusive","text":"<ul> <li>Seek to understand and respect different perspectives.</li> <li>Avoid discriminatory or exclusionary behavior.</li> </ul>"},{"location":"CODE_OF_CONDUCT/#be-collaborative","title":"Be Collaborative","text":"<ul> <li>Share knowledge and help others.</li> <li>Foster a welcoming environment for collaboration.</li> </ul>"},{"location":"CODE_OF_CONDUCT/#be-mindful","title":"Be Mindful","text":"<ul> <li>Be aware of your surroundings and of your fellow participants.</li> <li>Alert community leaders if you notice a dangerous situation or someone in distress.</li> </ul>"},{"location":"CODE_OF_CONDUCT/#unacceptable-behavior","title":"Unacceptable Behavior","text":"<p>Examples of unacceptable behavior include:</p> <ul> <li>Harassment, intimidation, or discrimination in any form.</li> <li>Offensive comments related to gender, sexual orientation, disability, ethnicity, religion, or age.</li> <li>The use of sexualized language or imagery.</li> <li>Unwelcome sexual attention or advances.</li> <li>Trolling, insulting, or derogatory comments.</li> <li>Public or private harassment.</li> <li>Publishing others\u2019 private information without explicit permission.</li> </ul>"},{"location":"CODE_OF_CONDUCT/#reporting-and-enforcement","title":"Reporting and Enforcement","text":""},{"location":"CODE_OF_CONDUCT/#reporting","title":"Reporting","text":"<p>If you are subject to or witness unacceptable behavior, or have any other concerns, please notify a community leader as soon as possible. You can report issues by:</p> <ul> <li>Emailing me at melis.emilio1@gmail.com</li> <li>Opening an issue on our GitHub repository</li> </ul>"},{"location":"CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Community leaders are responsible for clarifying and enforcing our standards. They may take appropriate and fair corrective action in response to any behavior they deem inappropriate, including:</p> <ul> <li>A warning to the offender.</li> <li>Temporary or permanent expulsion from the community without warning.</li> </ul>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.</p> <p>Thank you for helping to make this a welcoming, friendly, and productive community for everyone.</p>"},{"location":"CONTRIBUTING/","title":"Contributing to LightDiffusion","text":"<p>Thank you for considering contributing to LightDiffusion! Your help is greatly appreciated. By contributing, you help improve the project and make it more useful for everyone.</p>"},{"location":"CONTRIBUTING/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>How to Contribute</li> <li>Code of Conduct</li> <li>Getting Started</li> <li>Reporting Issues</li> <li>Submitting Changes</li> <li>Style Guide</li> <li>Community and Support</li> </ol>"},{"location":"CONTRIBUTING/#introduction","title":"Introduction","text":"<p>LightDiffusion is a cutting-edge tool designed to generate high-quality images from text prompts. This project aims to simplify and enhance the process of creating stunning visuals using advanced machine learning techniques.</p>"},{"location":"CONTRIBUTING/#how-to-contribute","title":"How to Contribute","text":"<p>There are several ways you can contribute to LightDiffusion:</p> <ul> <li>Reporting Bugs: If you find a bug, please report it by opening an issue.</li> <li>Suggesting Enhancements: If you have an idea for a new feature or an improvement, feel free to suggest it.</li> <li>Submitting Pull Requests: If you have a fix or a new feature, you can submit a pull request.</li> <li>Improving Documentation: Help us improve our documentation by suggesting changes or adding new content.</li> </ul>"},{"location":"CONTRIBUTING/#code-of-conduct","title":"Code of Conduct","text":"<p>Please read and follow our Code of Conduct to ensure a welcoming and inclusive environment for everyone.</p>"},{"location":"CONTRIBUTING/#getting-started","title":"Getting Started","text":"<p>To get started with contributing, follow these steps:</p> <ol> <li>Fork the Repository: Click the \u201cFork\u201d button at the top right of the repository page.</li> <li>Clone Your Fork: Clone your forked repository to your local machine.     <code>bash     git clone https://github.com/your-username/LightDiffusion.git     cd LightDiffusion</code></li> <li>Create a Branch: Create a new branch for your changes.     <code>bash     git checkout -b my-feature-branch</code></li> <li>Make Changes: Make your changes to the codebase.</li> <li>Commit Changes: Commit your changes with a descriptive commit message.     <code>bash     git commit -m \"Add new feature\"</code></li> <li>Push Changes: Push your changes to your forked repository.     <code>bash     git push origin my-feature-branch</code></li> <li>Open a Pull Request: Open a pull request to the main repository.</li> </ol>"},{"location":"CONTRIBUTING/#reporting-issues","title":"Reporting Issues","text":"<p>If you encounter any issues, please report them by opening an issue on the GitHub repository. Provide as much detail as possible to help us understand and resolve the issue.</p>"},{"location":"CONTRIBUTING/#submitting-changes","title":"Submitting Changes","text":"<p>When submitting changes, please ensure that you:</p> <ul> <li>Follow the project\u2019s coding standards and style guide.</li> <li>Write clear and concise commit messages.</li> <li>Include tests for any new features or bug fixes.</li> <li>Update the documentation if necessary.</li> </ul>"},{"location":"CONTRIBUTING/#style-guide","title":"Style Guide","text":"<p>To maintain consistency in the codebase, please follow these guidelines:</p> <ul> <li>Use meaningful variable and function names.</li> <li>Write comments to explain complex logic.</li> <li>Follow the PEP 8 style guide for Python code.</li> <li>Ensure your code is properly formatted and linted.</li> </ul>"},{"location":"CONTRIBUTING/#community-and-support","title":"Community and Support","text":"<p>Join our community on GitHub to share your creations, report issues, and contribute to the project. Check out the FAQ for common questions and troubleshooting tips.</p> <p>Thank you for contributing to LightDiffusion! Your support and contributions are greatly appreciated.</p>"},{"location":"DiscordBot/","title":"DiscordBot Installation Guide for LightDiffusion","text":"<p>Welcome to the Discord-Bot installation guide for LightDiffusion. Follow the steps below to set up the Discord-Bot on your system.</p>"},{"location":"DiscordBot/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following installed:</p> <ul> <li>Python 3.10.6</li> <li>Git</li> <li><code>py-cord</code> library</li> <li>A Discord bot token (stored in a <code>.env</code> file)</li> </ul>"},{"location":"DiscordBot/#installation-steps","title":"Installation Steps","text":""},{"location":"DiscordBot/#1-clone-the-repository","title":"1. Clone the Repository","text":"<p>Open your terminal and run the following command to clone the repository inside the LightDiffusion directory:</p> <pre><code>cd /path/to/LightDiffusion\ngit clone https://github.com/Aatrick/Boubou.git\ncd Boubou\n</code></pre>"},{"location":"DiscordBot/#2-install-dependencies","title":"2. Install Dependencies","text":"<p>With the LightDiffusion venv activated, install the required Python dependencies using <code>pip</code>:</p> <pre><code>pip install -r requirements.txt\npip install py-cord\n</code></pre>"},{"location":"DiscordBot/#3-set-up-the-environment-variables","title":"3. Set Up the Environment Variables","text":"<p>Create a <code>.env</code> file in the <code>Boubou</code> directory and add your Discord bot token:</p> <pre><code>TOKEN = your_discord_bot_token_here\n</code></pre>"},{"location":"DiscordBot/#4-run-the-bot","title":"4. Run the Bot","text":"<p>Execute the following command to start the Discord bot:</p> <pre><code>python bot.py\n</code></pre> <p></p>"},{"location":"DiscordBot/#tips-and-tricks","title":"Tips and Tricks","text":"<p>[!TIP] - Ensure your Discord bot has the necessary permissions to read and send messages in your server. - For best performance, run the bot on a server with a stable internet connection. - Refer to the Discord Developer Portal for more information on creating and managing your Discord bot.</p>"},{"location":"DiscordBot/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any issues during installation or usage, please refer to the FAQ or open an issue on the GitHub repository.</p> <p>Wish you good generations!</p>"},{"location":"Flux/","title":"Flux Installation Guide","text":"<p>Welcome to the Flux installation guide. Follow the steps below to set up Flux on your system.</p>"},{"location":"Flux/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following installed:</p> <ul> <li>Python 3.10.6</li> <li>Git</li> <li>At least 25GB of free space on your hard drive (40-50GB recommended)</li> </ul>"},{"location":"Flux/#installation-steps","title":"Installation Steps","text":""},{"location":"Flux/#1-clone-the-repository","title":"1. Clone the Repository","text":"<p>Open your terminal and run the following command to clone the repository:</p> <pre><code>git clone https://github.com/Aatrick/LightDiffusion.git\ncd LightDiffusion\n</code></pre>"},{"location":"Flux/#2-run-the-application","title":"2. Run the Application","text":""},{"location":"Flux/#21-windows","title":"2.1 Windows","text":"<p>Open a command prompt and execute the <code>run.bat</code> file to start the application:</p> <pre><code>./run.bat\n</code></pre>"},{"location":"Flux/#22-linux","title":"2.2 Linux","text":"<p>Open a terminal and execute the <code>run.sh</code> file to start the application:</p> <pre><code>./run.sh\n</code></pre>"},{"location":"Flux/#3-add-model-checkpoints","title":"3. Add Model Checkpoints","text":"<p>Download your Q8 Flux Dev model and place it in the <code>Unet</code> directory.</p>"},{"location":"Flux/#tips-and-tricks","title":"Tips and Tricks","text":"<p>[!TIP] - Ensure you have enough free space on your ram or storage to accommodate the model and generated images. - Ensure your system meets the minimum requirements for optimal performance. - For best results, use resolutions corresponding to megapixel sizes. Refer to this guide for more information. - If you encounter any issues with the prompt enhancer, try unchecking and rechecking the prompt enhancer checkbox in the GUI.</p>"},{"location":"Flux/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any issues during installation or usage, please refer to the FAQ or open an issue on the GitHub repository.</p> <p>Wish you good generations!</p>"},{"location":"HiresFix%20%26%20Adetailer/","title":"HiresFix and Adetailer Guide for LightDiffusion","text":"<p>Welcome to the HiresFix and Adetailer guide for LightDiffusion. This document will help you understand how to use these features to enhance your generated images.</p>"},{"location":"HiresFix%20%26%20Adetailer/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>HiresFix    - Overview    - Usage    - Tips and Tricks</li> <li>Adetailer    - Overview    - Usage    - Tips and Tricks</li> <li>Troubleshooting</li> </ol>"},{"location":"HiresFix%20%26%20Adetailer/#introduction","title":"Introduction","text":"<p>LightDiffusion offers powerful tools like HiresFix and Adetailer to improve the quality and details of your generated images. By leveraging these features, you can achieve higher resolution and more refined details in your outputs.</p>"},{"location":"HiresFix%20%26%20Adetailer/#hiresfix","title":"HiresFix","text":""},{"location":"HiresFix%20%26%20Adetailer/#overview","title":"Overview","text":"<p>HiresFix is a feature that enhances the resolution of your generated images. It uses advanced algorithms to upscale images while preserving details and minimizing artifacts.</p>"},{"location":"HiresFix%20%26%20Adetailer/#usage","title":"Usage","text":"<p>To use HiresFix, follow these steps:</p> <ol> <li>Enable HiresFix: In the LightDiffusion GUI, check the HiresFix option.</li> <li>Run HiresFix: Click the \u201cGenerate\u201d button to apply HiresFix during your generation.</li> </ol> <p>[!Note] You may need to adjust the upscaling factor and denoising strength based on your image and desired output. those settings are available at the bottom of the <code>LightDiffusion.py</code> file.</p>"},{"location":"HiresFix%20%26%20Adetailer/#tips-and-tricks","title":"Tips and Tricks","text":"<ul> <li>Experiment with Upscaling Factors: Different images may require different upscaling factors. Experiment to find the best setting for your image.</li> <li>Adjust Denoising Strength: Higher denoising strength can help reduce artifacts but may also blur details. Find a balance that works for your image.</li> <li>Use High-Quality Base Images: Starting with a high-quality base image can yield better results after applying HiresFix.</li> </ul>"},{"location":"HiresFix%20%26%20Adetailer/#adetailer","title":"Adetailer","text":""},{"location":"HiresFix%20%26%20Adetailer/#overview_1","title":"Overview","text":"<p>Adetailer is a feature that enhances specific details in your generated images, such as faces, eyes, body, and other intricate elements. It uses different bbox yolov9 models to detect and enhance these details, resulting in more refined and realistic outputs.</p>"},{"location":"HiresFix%20%26%20Adetailer/#usage_1","title":"Usage","text":"<p>To use Adetailer, follow these steps:</p> <ol> <li>Enable Adetailer: In the LightDiffusion GUI, check the Adetailer option.</li> <li>Run Adetailer: Click the \u201cGenerate\u201d button to apply Adetailer during your generation.</li> </ol>"},{"location":"HiresFix%20%26%20Adetailer/#tips-and-tricks_1","title":"Tips and Tricks","text":"<ul> <li>Generate humans: Adetailer works best with images containing humans or human-like elements.</li> <li>Adjust Enhancement Strength: Higher enhancement strength can bring out more details but may also introduce artifacts. Find a balance that works for your image.</li> <li>Combine with HiresFix: For the best results, use Adetailer in combination with HiresFix to achieve both high resolution and detailed enhancements.</li> </ul>"},{"location":"HiresFix%20%26%20Adetailer/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues with HiresFix or Adetailer, consider the following:</p> <ul> <li>Check for Typos: Ensure there are no typos in your prompt or parameter settings.</li> <li>Simplify Your Prompt: Start with a simple prompt and gradually add complexity.</li> <li>Adjust Parameters: Experiment with different parameter settings to find the best results.</li> <li>Refer to the FAQ: Check the FAQ for common issues and solutions.</li> </ul> <p>Wish you good generations!</p>"},{"location":"Img2Img/","title":"Img2Img Guide for LightDiffusion","text":"<p>Welcome to the Img2Img guide for LightDiffusion. This document will help you understand how to use the Img2Img feature in LightDiffusion to transform existing images into new, high-quality visuals.</p>"},{"location":"Img2Img/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Prerequisites</li> <li>Basic Usage</li> <li>Tips and Tricks</li> <li>Troubleshooting</li> </ol>"},{"location":"Img2Img/#introduction","title":"Introduction","text":"<p>Img2Img is a powerful feature in LightDiffusion that allows you to generate new images based on existing ones. By providing a source image and a text prompt, you can create stunning transformations and enhancements.</p>"},{"location":"Img2Img/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following:</p> <ul> <li>A source image that you want to transform.</li> <li>Basic understanding of how to create prompts in LightDiffusion. Refer to the Prompting Guide for more information.</li> </ul>"},{"location":"Img2Img/#basic-usage","title":"Basic Usage","text":""},{"location":"Img2Img/#step-1-prepare-your-source-image","title":"Step 1: Prepare Your Source Image","text":"<p>Ensure your source image is in a supported format (e.g., PNG, JPEG) and is placed in an accessible directory.</p>"},{"location":"Img2Img/#step-2-create-your-prompt","title":"Step 2: Create Your Prompt","text":"<p>Write down a prompt made mostly of quality descriptors or styles that you want to see in the generated image. Refer to the Prompting Guide for more information.</p>"},{"location":"Img2Img/#step-3-run-img2img","title":"Step 3: Run Img2Img","text":"<p>Click on the Img2Img button in the LightDiffusion GUI and select your source image. The model will generate a new image based on the source image and prompt.</p>"},{"location":"Img2Img/#tips-and-tricks","title":"Tips and Tricks","text":"<ul> <li>Experiment with CFG Scale: The CFG scale controls the adherence to the prompt. Higher values result in more adherence but can lead to overfitting.</li> <li>Use High-Quality Descriptions: Detailed and specific descriptions yield better results.</li> <li>Leverage Negative Prompts: Use negative prompts to filter out unwanted elements.</li> <li>Iterate and Refine: Generate multiple images and refine your prompts based on the results.</li> </ul>"},{"location":"Img2Img/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues with Img2Img, consider the following:</p> <ul> <li>Check for Typos: Ensure there are no typos in your prompt.</li> <li>Simplify Your Prompt: Start with a simple prompt and gradually add complexity.</li> <li>Adjust CFG Scale: Experiment with different CFG scale values.</li> <li>Refer to the FAQ: Check the FAQ for common issues and solutions.</li> </ul> <p>Wish you good generations!</p>"},{"location":"Installation/","title":"Installation Guide","text":"<p>Welcome to the LightDiffusion installation guide. Follow the steps below to set up LightDiffusion on your system.</p>"},{"location":"Installation/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following installed:</p> <ul> <li>Python 3.10.6</li> <li>Git</li> <li>At least 10GB of free space on your hard drive</li> </ul>"},{"location":"Installation/#installation-steps","title":"Installation Steps","text":""},{"location":"Installation/#1-clone-the-repository","title":"1. Clone the Repository","text":"<p>Open your terminal and run the following command to clone the repository:</p> <pre><code>git clone https://github.com/Aatrick/LightDiffusion.git\ncd LightDiffusion\n</code></pre>"},{"location":"Installation/#2-run-the-application","title":"2. Run the Application","text":""},{"location":"Installation/#21-windows","title":"2.1 Windows","text":"<p>Open a command prompt and execute the <code>run.bat</code> file to start the application:</p> <pre><code>./run.bat\n</code></pre>"},{"location":"Installation/#22-linux","title":"2.2 Linux","text":"<p>Open a terminal and execute the <code>run.sh</code> file to start the application:</p> <pre><code>./run.sh\n</code></pre>"},{"location":"Installation/#3-add-model-checkpoints","title":"3. Add Model Checkpoints","text":"<p>Download your SD1/1.5 safetensors model and place it in the <code>checkpoints</code> directory.</p> <p>[!TIP] - Ensure your system meets the minimum requirements for optimal performance. - For best results, use resolutions corresponding to megapixel sizes. Refer to this guide for more information.</p>"},{"location":"Installation/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any issues during installation or usage, please refer to the FAQ or open an issue on the GitHub repository.</p> <p>Wish you good generations!</p>"},{"location":"LoRas/","title":"LoRas Guide for LightDiffusion","text":"<p>Welcome to the LoRas guide for LightDiffusion. This document will help you understand how to use LoRas (Low-Rank Adaptations) to enhance your image generation process.</p>"},{"location":"LoRas/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>What are LoRas?</li> <li>Loading LoRas</li> <li>Using LoRas</li> <li>Advanced Techniques</li> <li>Troubleshooting</li> <li>FAQ</li> </ol>"},{"location":"LoRas/#introduction","title":"Introduction","text":"<p>LightDiffusion offers powerful tools like LoRas to improve the quality and details of your generated images. By leveraging these features, you can achieve higher resolution and more refined details in your outputs.</p>"},{"location":"LoRas/#what-are-loras","title":"What are LoRas?","text":"<p>LoRas (Low-Rank Adaptations) are a technique used to fine-tune pre-trained models with a smaller number of parameters. This allows for efficient adaptation of models to new tasks or domains without the need for extensive retraining.</p>"},{"location":"LoRas/#loading-loras","title":"Loading LoRas","text":"<p>To load LoRas in LightDiffusion, follow these steps:</p> <ol> <li> <p>Prepare Your LoRas: Ensure your LoRas files are in the correct format and located in the appropriate directory (<code>./_internal/loras</code>).</p> </li> <li> <p>Load LoRas in the GUI:</p> </li> </ol> <ul> <li>Open the LightDiffusion GUI.</li> <li>Select the wanted LoRas from the dropdown menu.</li> </ul> <ol> <li>Verify Loaded LoRas: Check if the LoRas have been loaded correctly by inspecting the loaded keys and parameters.</li> </ol>"},{"location":"LoRas/#using-loras","title":"Using LoRas","text":"<p>Once loaded, you can use LoRas to enhance your image generation process. Here\u2019s how:</p> <ol> <li> <p>Enhance Prompts: Apply LoRas to enhance your prompts by selecting the desired LoRas in the GUI.</p> </li> <li> <p>Generate Images: Use the enhanced model to generate images with improved quality and details.</p> </li> </ol>"},{"location":"LoRas/#advanced-techniques","title":"Advanced Techniques","text":""},{"location":"LoRas/#combining-multiple-loras","title":"Combining Multiple LoRas","text":"<p>You can combine multiple LoRas to achieve more complex adaptations. Ensure that the LoRas are compatible and do not conflict with each other. To combine multiple LoRas, load them together and apply them sequentially to your model by modifying the execution pipeline at the bottom of the <code>LightDiffusion.py</code> file.</p>"},{"location":"LoRas/#fine-tuning-loras","title":"Fine-Tuning LoRas","text":"<p>For advanced users, fine-tuning LoRas can provide even better results. Experiment with different parameters and configurations to find the optimal settings for your tasks.</p>"},{"location":"LoRas/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues with LoRas, consider the following:</p> <ul> <li>Check File Formats: Ensure your LoRas files are in the correct format (<code>.safetensors</code>).</li> <li>Verify Directory Paths: Confirm that the LoRas files are located in the correct directory (<code>./_internal/loras</code>).</li> <li>Inspect Logs: Check the logs for detailed information on what might be going wrong.</li> </ul>"},{"location":"LoRas/#faq","title":"FAQ","text":"<p>Q: What are the supported formats for LoRas?</p> <p>A: LightDiffusion supports <code>.safetensors</code> format for LoRas.</p> <p>Q: How do I combine multiple LoRas?</p> <p>A: You can combine multiple LoRas by loading them together and applying them to your model.</p> <p>Q: Where can I find more information?</p> <p>A: For more information, refer to the LightDiffusion documentation and the FAQ.</p> <p>Thank you for using LightDiffusion! We hope this guide helps you create amazing visuals with enhanced prompts.</p>"},{"location":"Ollama/","title":"Ollama Integration Guide for LightDiffusion","text":"<p>Welcome to the Ollama integration guide for LightDiffusion. This document will help you understand how to set up and use the Ollama prompt enhancer within LightDiffusion to improve your image generation prompts.</p>"},{"location":"Ollama/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Prerequisites</li> <li>Installation</li> <li>Usage</li> <li>Tips and Tricks</li> <li>Troubleshooting</li> <li>FAQ</li> </ol>"},{"location":"Ollama/#introduction","title":"Introduction","text":"<p>Ollama is a powerful tool that enhances your text prompts to generate higher quality images with LightDiffusion. By integrating Ollama, you can leverage advanced algorithms to automatically refine and optimize your prompts.</p>"},{"location":"Ollama/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following:</p> <ul> <li>LightDiffusion installed on your system. Refer to the Installation Guide for detailed instructions.</li> <li>Python 3.10.6 or later.</li> <li>Internet connection for downloading necessary packages and models.</li> </ul>"},{"location":"Ollama/#installation","title":"Installation","text":"<p>To install the Ollama prompt enhancer, follow these steps:</p> <ol> <li> <p>Use the official installer</p> <p>Download the installer according to your operating system on the official website</p> </li> <li> <p>Run Ollama</p> <p><code>bash ollama run llama3.2</code></p> </li> </ol>"},{"location":"Ollama/#usage","title":"Usage","text":"<p>Once Ollama is configured, you can start using it to enhance your prompts. Here\u2019s how:</p> <ol> <li> <p>Open LightDiffusion</p> <p>Launch LightDiffusion by running the following command:</p> <p><code>bash ./run.bat  # For Windows ./run.sh   # For Linux</code></p> </li> <li> <p>Enable Prompt Enhancer</p> <p>In the LightDiffusion GUI, navigate to the settings and enable the prompt enhancer checkbox.</p> </li> <li> <p>Generate Images</p> <p>Create your text prompts as usual. Ollama will automatically enhance your prompts to generate higher quality images.</p> </li> </ol>"},{"location":"Ollama/#tips-and-tricks","title":"Tips and Tricks","text":"<ul> <li>Experiment with Different Models: Ollama supports various models. Experiment with different models to find the one that works best for your needs.</li> <li>Simplify Prompts: Start with simple prompts and gradually add complexity. This helps in understanding how Ollama enhances your prompts.</li> <li>Check Logs: If you encounter issues, check the logs for detailed information on what might be going wrong.</li> </ul>"},{"location":"Ollama/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues with Ollama, consider the following:</p> <ul> <li>Check Configuration: Ensure the configuration file is correctly set up.</li> <li>Restart LightDiffusion: Sometimes, a simple restart can resolve issues.</li> <li>Update Dependencies: Ensure all dependencies are up to date by running <code>pip install --upgrade -r requirements.txt</code>.</li> </ul>"},{"location":"Ollama/#faq","title":"FAQ","text":"<p>Q: What models does Ollama support?</p> <p>A: Ollama supports various models, including llama3.2. Refer to the Ollama documentation for a complete list.</p> <p>Q: How do I disable the prompt enhancer?</p> <p>A: You can disable the prompt enhancer by unchecking the prompt enhancer checkbox in the LightDiffusion GUI.</p> <p>Q: Where can I find more information?</p> <p>A: For more information, refer to the LightDiffusion documentation and the Ollama documentation.</p> <p>Thank you for using LightDiffusion with Ollama! We hope this guide helps you create amazing visuals with enhanced prompts.</p>"},{"location":"Prompting/","title":"Prompting Guide for LightDiffusion","text":"<p>Welcome to the LightDiffusion Prompting Guide. This document will help you understand how to create effective prompts for generating high-quality images using LightDiffusion.</p>"},{"location":"Prompting/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Basic Prompt Structure</li> <li>Advanced Prompt Techniques</li> <li>Prompt Examples</li> <li>Tips and Tricks</li> <li>Troubleshooting</li> </ol>"},{"location":"Prompting/#introduction","title":"Introduction","text":"<p>LightDiffusion is a powerful tool for generating images from text prompts. By understanding how to structure your prompts, you can create stunning visuals with ease.</p>"},{"location":"Prompting/#basic-prompt-structure","title":"Basic Prompt Structure","text":"<p>A basic prompt in LightDiffusion consists of a description of the desired image. Here is the general structure of the <code>prompt.txt</code> file :</p> <pre><code>prompt: &lt;description of the image&gt;\nneg: &lt;negative prompts&gt;\nw: &lt;width&gt;\nh: &lt;height&gt;\ncfg: &lt;cfg scale&gt;\n</code></pre>"},{"location":"Prompting/#example","title":"Example","text":"<pre><code>prompt: a beautiful sunset over the mountains, vibrant colors, high resolution\nneg: low quality, blurry\nw: 1024\nh: 768\ncfg: 7\n</code></pre>"},{"location":"Prompting/#advanced-prompt-techniques","title":"Advanced Prompt Techniques","text":""},{"location":"Prompting/#using-parentheses-for-emphasis","title":"Using Parentheses for Emphasis","text":"<p>You can use parentheses to emphasize certain parts of your prompt. The more parentheses you use, the more emphasis is placed on that part of the prompt.</p> <pre><code>prompt: ((a beautiful sunset)), over the mountains, (vibrant colors), high resolution\n</code></pre>"},{"location":"Prompting/#using-colons-for-weight","title":"Using Colons for Weight","text":"<p>You can use colons to increase the weight of certain parts of your prompt. The higher the weight, the more importance is given to that part of the prompt.</p> <pre><code>prompt: (a beautiful sunset:1.2), over the mountains, vibrant colors, (high resolution:1.5)\n</code></pre>"},{"location":"Prompting/#combining-multiple-concepts","title":"Combining Multiple Concepts","text":"<p>You can combine multiple concepts in a single prompt to create more complex images.</p> <pre><code>prompt: a futuristic cityscape, neon lights, flying cars, (cyberpunk style)\n</code></pre> <p>[!TIP] Use High-Quality Descriptions: Detailed and specific descriptions yield better results.</p>"},{"location":"Prompting/#negative-prompts","title":"Negative Prompts","text":"<p>Negative prompts help you avoid unwanted elements in your generated images.</p> <pre><code>neg: low quality, blurry, distorted\n</code></pre> <p>[!TIP] Experiment with CFG Scale: The CFG scale controls the adherence to the prompt. Higher values result in more adherence but can lead to overfitting.</p>"},{"location":"Prompting/#quality-tokens","title":"Quality tokens","text":"<p>Being trained on image datasets, you can use quality tokens/keywords to improve the quality of the generated images.</p> <pre><code>masterpiece, ****,  best quality, (extremely detailed CG unity 8k wallpaper, ultra-detailed, best shadow), (beautiful detailed face, beautiful detailed eyes), High contrast, ((colourful paint splashes on transparent background, dulux,)), ((caustic)), dynamic angle, beautiful detailed glow, wallpaper, (detailed background), (best illumination, an extremely delicate and beautiful), hyper detail, intricate details,\n</code></pre>"},{"location":"Prompting/#embeddings","title":"Embeddings","text":"<p>You can use embeddings in the negative prompts to have premade negative prompts, reducing the need to write them out.</p> <pre><code>neg: (embedding:EasyNegative), (embedding:badhandv4:1.2)\n</code></pre>"},{"location":"Prompting/#camera-angles","title":"Camera Angles","text":"<p>You can specify the camera angle in your prompt to control the perspective of the generated image.</p> <pre><code>dynamic angle, top-down view, bird's eye view, close-up shot, wide-angle shot, pov, third-person perspective, panoramic view, cowboy shot, full body, looking at viewer, ...\n</code></pre>"},{"location":"Prompting/#lighting","title":"Lighting","text":"<p>You can specify the lighting conditions in your prompt to achieve the desired effect.</p> <pre><code>best illumination, cinematic light, dramatic light, beautiful glow, chromatic aberration, high contrast, dynamic lighting, soft shadows, warm light, cool light, natural light, ...\n</code></pre>"},{"location":"Prompting/#flux","title":"Flux","text":"<p>Prompting for Flux is a bit different than LightDiffusion, since you don\u2019t have a negative prompt. You should start your prompt with a phrase in natural language, and then add the quality tokens and other details.</p> <pre><code>prompt: a beautiful sunset over the mountains, vibrant colors, high resolution, (best quality, ultra-detailed, high contrast)\n</code></pre>"},{"location":"Prompting/#prompt-examples","title":"Prompt Examples","text":""},{"location":"Prompting/#simple-prompt","title":"Simple Prompt","text":"<pre><code>prompt: a serene beach at sunrise\nneg: low quality, dark\nw: 800\nh: 600\ncfg: 6\n</code></pre>"},{"location":"Prompting/#detailed-prompt","title":"Detailed Prompt","text":"<pre><code>prompt: (a majestic dragon), flying over a medieval castle, (high detail), (fantasy art)\nneg: cartoonish, low quality\nw: 1024\nh: 1024\ncfg: 8\n</code></pre>"},{"location":"Prompting/#artistic-style","title":"Artistic Style","text":"<pre><code>prompt: a portrait of a woman, (in the style of Van Gogh), (impressionist painting)\nneg: low quality, abstract\nw: 768\nh: 1024\ncfg: 7\n</code></pre>"},{"location":"Prompting/#advanced","title":"Advanced","text":"<pre><code>prompt: masterpiece, best quality, (extremely detailed CG unity 8k wallpaper, masterpiece, best quality, ultra-detailed, best shadow), (detailed background), (beautiful detailed face, beautiful detailed eyes), High contrast, (best illumination, an extremely delicate and beautiful),1girl,((colourful paint splashes on transparent background, dulux,)), ((caustic)), dynamic angle,beautiful detailed glow,full body, cowboy shot, colorful, 1girl, white hair, purple eyes, dual wielding, sword, holding sword, blue flames, glow, glowing weapon, light particles, wallpaper, chromatic aberration, (detailed background,dark fantasy), (beautiful detailed face), high contrast, (best illumination, an extremely delicate and beautiful), ((cinematic light)), colorful, hyper detail, dramatic light, intricate details, depth of field,black light particles,(broken glass),magic circle\n</code></pre>"},{"location":"Prompting/#tips-and-tricks","title":"Tips and Tricks","text":"<ul> <li>Experiment with CFG Scale: The CFG scale controls the adherence to the prompt. Higher values result in more adherence but can lead to overfitting.</li> <li>Use High-Quality Descriptions: Detailed and specific descriptions yield better results.</li> <li>Leverage Negative Prompts: Use negative prompts to filter out unwanted elements.</li> <li>Iterate and Refine: Generate multiple images and refine your prompts based on the results.</li> </ul>"},{"location":"Prompting/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues with your prompts, consider the following:</p> <ul> <li>Check for Typos: Ensure there are no typos in your prompt.</li> <li>Simplify Your Prompt: Start with a simple prompt and gradually add complexity.</li> <li>Adjust CFG Scale: Experiment with different CFG scale values.</li> <li>Refer to the FAQ: Check the FAQ for common issues and solutions.</li> </ul> <p>Wish you good generations!</p>"},{"location":"Stable-fast/","title":"Stable-fast","text":"<p>Royalty of the Stable-fast.md</p>"},{"location":"Stable-fast/#introduction","title":"Introduction","text":""},{"location":"Stable-fast/#what-is-this","title":"What is this?","text":"<p><code>stable-fast</code> is an ultra lightweight inference optimization framework for HuggingFace Diffusers on NVIDIA GPUs. <code>stable-fast</code> provides super fast inference optimization by utilizing some key techniques and features:</p> <ul> <li>CUDNN Convolution Fusion: <code>stable-fast</code> implements a series of fully-functional and fully-compatible CUDNN convolution fusion operators for all kinds of combinations of <code>Conv + Bias + Add + Act</code> computation patterns.</li> <li>Low Precision &amp; Fused GEMM: <code>stable-fast</code> implements a series of fused GEMM operators that compute with <code>fp16</code> precision, which is fast than PyTorch\u2019s defaults (read &amp; write with <code>fp16</code> while compute with <code>fp32</code>).</li> <li>Fused Linear GEGLU: <code>stable-fast</code> is able to fuse <code>GEGLU(x, W, V, b, c) = GELU(xW + b) \u2297 (xV + c)</code> into one CUDA kernel.</li> <li>NHWC &amp; Fused GroupNorm: <code>stable-fast</code> implements a highly optimized fused NHWC <code>GroupNorm + Silu</code> operator with OpenAI\u2019s <code>Triton</code>, which eliminates the need of memory format permutation operators.</li> <li>Fully Traced Model: <code>stable-fast</code> improves the <code>torch.jit.trace</code> interface to make it more proper for tracing complex models. Nearly every part of <code>StableDiffusionPipeline/StableVideoDiffusionPipeline</code> can be traced and converted to TorchScript. It is more stable than <code>torch.compile</code> and has a significantly lower CPU overhead than <code>torch.compile</code> and supports ControlNet and LoRA.</li> <li>CUDA Graph: <code>stable-fast</code> can capture the <code>UNet</code>, <code>VAE</code> and <code>TextEncoder</code> into CUDA Graph format, which can reduce the CPU overhead when the batch size is small. This implemention also supports dynamic shape.</li> <li>Fused Multihead Attention: <code>stable-fast</code> just uses xformers and makes it compatible with TorchScript.</li> </ul> <p>My next goal is to keep <code>stable-fast</code> as one of the fastest inference optimization frameworks for <code>diffusers</code> and also provide both speedup and VRAM reduction for <code>transformers</code>. In fact, I already use <code>stable-fast</code> to optimize LLMs and achieve a significant speedup. But I still need to do some work to make it more stable and easy to use and provide a stable user interface.</p>"},{"location":"Stable-fast/#differences-with-other-acceleration-libraries","title":"Differences With Other Acceleration Libraries","text":"<ul> <li>Fast: <code>stable-fast</code> is specialy optimized for HuggingFace Diffusers. It achieves a high performance across many libraries. And it provides a very fast compilation speed within only a few seconds. It is significantly faster than <code>torch.compile</code>, <code>TensorRT</code> and <code>AITemplate</code> in compilation time.</li> <li>Minimal: <code>stable-fast</code> works as a plugin framework for <code>PyTorch</code>. It utilizes existing <code>PyTorch</code> functionality and infrastructures and is compatible with other acceleration techniques, as well as popular fine-tuning techniques and deployment solutions.</li> <li>Maximum Compatibility: <code>stable-fast</code> is compatible with all kinds of <code>HuggingFace Diffusers</code> and <code>PyTorch</code> versions. It is also compatible with <code>ControlNet</code> and <code>LoRA</code>. And it even supports the latest <code>StableVideoDiffusionPipeline</code> out of the box!</li> </ul>"},{"location":"Stable-fast/#installation","title":"Installation","text":"<p>NOTE: <code>stable-fast</code> is currently only tested on <code>Linux</code> and <code>WSL2 in Windows</code>. You need to install PyTorch with CUDA support at first (versions from 1.12 to 2.1 are suggested).</p> <p>I only test <code>stable-fast</code> with <code>torch&gt;=2.1.0</code>, <code>xformers&gt;=0.0.22</code> and <code>triton&gt;=2.1.0</code> on <code>CUDA 12.1</code> and <code>Python 3.10</code>. Other versions might build and run successfully but that\u2019s not guaranteed.</p>"},{"location":"Stable-fast/#install-prebuilt-wheels","title":"Install Prebuilt Wheels","text":"<p>Download the wheel corresponding to your system from the Releases Page and install it with <code>pip3 install &lt;wheel file&gt;</code>.</p> <p>Currently both Linux and Windows wheels are available.</p> <pre><code># Change cu121 to your CUDA version and &lt;wheel file&gt; to the path of the wheel file.\n# And make sure the wheel file is compatible with your PyTorch version.\npip3 install --index-url https://download.pytorch.org/whl/cu121 \\\n    'torch&gt;=2.1.0' 'xformers&gt;=0.0.22' 'triton&gt;=2.1.0' 'diffusers&gt;=0.19.3' \\\n    '&lt;wheel file&gt;'\n</code></pre>"},{"location":"Stable-fast/#install-from-source","title":"Install From Source","text":"<pre><code># Make sure you have CUDNN/CUBLAS installed.\n# https://developer.nvidia.com/cudnn\n# https://developer.nvidia.com/cublas\n\n# Install PyTorch with CUDA and other packages at first.\n# Windows user: Triton might be not available, you could skip it.\n# NOTE: 'wheel' is required or you will meet `No module named 'torch'` error when building.\npip3 install wheel 'torch&gt;=2.1.0' 'xformers&gt;=0.0.22' 'triton&gt;=2.1.0' 'diffusers&gt;=0.19.3'\n\n# (Optional) Makes the build much faster.\npip3 install ninja\n\n# Set TORCH_CUDA_ARCH_LIST if running and building on different GPU types.\n# You can also install the latest stable release from PyPI.\n# pip3 install -v -U stable-fast\npip3 install -v -U git+https://github.com/chengzeyi/stable-fast.git@main#egg=stable-fast\n# (this can take dozens of minutes)\n</code></pre> <p>NOTE: Any usage outside <code>sfast.compilers</code> is not guaranteed to be backward compatible.</p> <p>NOTE: To get the best performance, <code>xformers</code> and OpenAI\u2019s <code>triton&gt;=2.1.0</code> need to be installed and enabled. You might need to build <code>xformers</code> from source to make it compatible with your <code>PyTorch</code>.</p>"}]}